{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load NLP modules\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_business = pd.read_csv('../../data/dataset/csv/business.csv', index_col = 'business_id')\n",
    "#df_checkin = pd.read_csv('../../data/dataset/csv/checkin.csv', index_col = 'business_id')\n",
    "#df_photos = pd.read_csv('../../data/dataset/csv/photos.csv', index_col = 'photo_id')\n",
    "df_review = pd.read_csv('../../data/dataset/csv/review.csv', index_col = 'review_id')\n",
    "#df_tip = pd.read_csv('../../data/dataset/csv/tip.csv') # index not set for the df_tip\n",
    "df_user = pd.read_csv('../../data/dataset/csv/user.csv', index_col = 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 1-gram...\n",
      "Generate 2-gram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huawei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 3-gram...\n",
      "Generate basic text count features...\n",
      "done with train set preprocessing!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>useful</th>\n",
       "      <th>stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>bstar_ave</th>\n",
       "      <th>breview_count</th>\n",
       "      <th>fans</th>\n",
       "      <th>ureview_count</th>\n",
       "      <th>nfriends</th>\n",
       "      <th>Count_text_1gram</th>\n",
       "      <th>CountUnique_text_1gram</th>\n",
       "      <th>RatioUnique_text_1gram</th>\n",
       "      <th>Count_text_2gram</th>\n",
       "      <th>CountUnique_text_2gram</th>\n",
       "      <th>RatioUnique_text_2gram</th>\n",
       "      <th>Count_text_3gram</th>\n",
       "      <th>CountUnique_text_3gram</th>\n",
       "      <th>RatioUnique_text_3gram</th>\n",
       "      <th>CountDigit_text</th>\n",
       "      <th>RatioDigit_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PiOoO6Mn3jHG8-9-Dd85PA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>64</td>\n",
       "      <td>51</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UsCISuucrmqwqYds-CgyIg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>315</td>\n",
       "      <td>154</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>314</td>\n",
       "      <td>285</td>\n",
       "      <td>0.907643</td>\n",
       "      <td>313</td>\n",
       "      <td>306</td>\n",
       "      <td>0.977636</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.015873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HitVCYoau7GqGnexA6LHLQ</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>273</td>\n",
       "      <td>149</td>\n",
       "      <td>0.545788</td>\n",
       "      <td>272</td>\n",
       "      <td>252</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>271</td>\n",
       "      <td>266</td>\n",
       "      <td>0.981550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JqlEkwdtaYEcEDnSA9_DPg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>32</td>\n",
       "      <td>31</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AoES3CCk4t20097br47UMw</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>577.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3354.0</td>\n",
       "      <td>108</td>\n",
       "      <td>83</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        useful  stars  cool  funny  bstar_ave  breview_count  \\\n",
       "review_id                                                                      \n",
       "PiOoO6Mn3jHG8-9-Dd85PA     0.0    1.0   0.0    0.0        2.5           27.0   \n",
       "UsCISuucrmqwqYds-CgyIg     0.0    4.0   0.0    0.0        4.5          577.0   \n",
       "HitVCYoau7GqGnexA6LHLQ     2.0    2.0   0.0    0.0        2.5           98.0   \n",
       "JqlEkwdtaYEcEDnSA9_DPg     0.0    5.0   0.0    0.0        4.5          577.0   \n",
       "AoES3CCk4t20097br47UMw     0.0    5.0   0.0    0.0        4.5          577.0   \n",
       "\n",
       "                        fans  ureview_count  nfriends  Count_text_1gram  \\\n",
       "review_id                                                                 \n",
       "PiOoO6Mn3jHG8-9-Dd85PA   0.0           26.0      26.0                64   \n",
       "UsCISuucrmqwqYds-CgyIg   0.0           45.0     338.0               315   \n",
       "HitVCYoau7GqGnexA6LHLQ   0.0           45.0     338.0               273   \n",
       "JqlEkwdtaYEcEDnSA9_DPg   0.0           39.0     416.0                33   \n",
       "AoES3CCk4t20097br47UMw   2.0          106.0    3354.0               108   \n",
       "\n",
       "                        CountUnique_text_1gram  RatioUnique_text_1gram  \\\n",
       "review_id                                                                \n",
       "PiOoO6Mn3jHG8-9-Dd85PA                      51                0.796875   \n",
       "UsCISuucrmqwqYds-CgyIg                     154                0.488889   \n",
       "HitVCYoau7GqGnexA6LHLQ                     149                0.545788   \n",
       "JqlEkwdtaYEcEDnSA9_DPg                      28                0.848485   \n",
       "AoES3CCk4t20097br47UMw                      83                0.768519   \n",
       "\n",
       "                        Count_text_2gram  CountUnique_text_2gram  \\\n",
       "review_id                                                          \n",
       "PiOoO6Mn3jHG8-9-Dd85PA                63                      63   \n",
       "UsCISuucrmqwqYds-CgyIg               314                     285   \n",
       "HitVCYoau7GqGnexA6LHLQ               272                     252   \n",
       "JqlEkwdtaYEcEDnSA9_DPg                32                      31   \n",
       "AoES3CCk4t20097br47UMw               107                     107   \n",
       "\n",
       "                        RatioUnique_text_2gram  Count_text_3gram  \\\n",
       "review_id                                                          \n",
       "PiOoO6Mn3jHG8-9-Dd85PA                1.000000                62   \n",
       "UsCISuucrmqwqYds-CgyIg                0.907643               313   \n",
       "HitVCYoau7GqGnexA6LHLQ                0.926471               271   \n",
       "JqlEkwdtaYEcEDnSA9_DPg                0.968750                31   \n",
       "AoES3CCk4t20097br47UMw                1.000000               106   \n",
       "\n",
       "                        CountUnique_text_3gram  RatioUnique_text_3gram  \\\n",
       "review_id                                                                \n",
       "PiOoO6Mn3jHG8-9-Dd85PA                      62                1.000000   \n",
       "UsCISuucrmqwqYds-CgyIg                     306                0.977636   \n",
       "HitVCYoau7GqGnexA6LHLQ                     266                0.981550   \n",
       "JqlEkwdtaYEcEDnSA9_DPg                      31                1.000000   \n",
       "AoES3CCk4t20097br47UMw                     106                1.000000   \n",
       "\n",
       "                        CountDigit_text  RatioDigit_text  \n",
       "review_id                                                 \n",
       "PiOoO6Mn3jHG8-9-Dd85PA              1.0         0.015625  \n",
       "UsCISuucrmqwqYds-CgyIg              5.0         0.015873  \n",
       "HitVCYoau7GqGnexA6LHLQ              0.0         0.000000  \n",
       "JqlEkwdtaYEcEDnSA9_DPg              0.0         0.000000  \n",
       "AoES3CCk4t20097br47UMw              0.0         0.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.utils\n",
    "\n",
    "class PreProcessing:\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    # The constructor takes a pandas dataframe as input and save it to self.df\n",
    "    def __init__(self, df_r, df_b, df_u):\n",
    "        self.df_r = df_r\n",
    "        self.df_b = df_b\n",
    "        self.df_u = df_u\n",
    "        self.token_pattern = \"[a-zA-Z0-9'`]+\"\n",
    "\n",
    "\n",
    "    def TextPreProcessing(self, line):\n",
    "        ## tokenize\n",
    "        tokenizer = RegexpTokenizer(self.token_pattern)\n",
    "        tokens = tokenizer.tokenize( str(line).lower() )\n",
    "        return tokens\n",
    "\n",
    "    def Myngrams(self, text, nfold):\n",
    "        s = []\n",
    "        for ngram in ngrams(text, nfold):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "\n",
    "        #s = list(set(s)) # unique string in the list\n",
    "        return s\n",
    "\n",
    "    def TryDivide(self, x, y, val=0.0):\n",
    "        \"\"\" \n",
    "        Try to divide two numbers\n",
    "        \"\"\"\n",
    "        if y != 0.0:\n",
    "            val = float(x) / y\n",
    "        return val\n",
    "\n",
    "    def GetPositionList(self, tgt, obs):\n",
    "        \"\"\"\n",
    "        Get the list of positions of obs in target\n",
    "        \"\"\"\n",
    "        pos_of_obs_in_tgt = [0]\n",
    "        if len(obs) != 0:\n",
    "            pos_of_obs_in_tgt = [j for j,w in enumerate(obs, start=1) if w in tgt]\n",
    "            if len(pos_of_obs_in_tgt) == 0:\n",
    "                pos_of_obs_in_tgt = [0]\n",
    "        #print(pos_of_obs_in_tgt)\n",
    "        return pos_of_obs_in_tgt\n",
    "\n",
    "    def DumpTextBasicNgram(self, df):\n",
    "        ## 1-gram\n",
    "        print(\"Generate 1-gram...\")\n",
    "        df[\"text_1gram\"] = list(df.apply(lambda x: self.Myngrams(self.TextPreProcessing(x[\"text\"]), 1), axis=1))\n",
    "        ## 2-gram\n",
    "        print(\"Generate 2-gram...\")\n",
    "        df[\"text_2gram\"] = list(df.apply(lambda x: self.Myngrams(self.TextPreProcessing(x[\"text\"]), 2), axis=1))\n",
    "        ## 3-gram\n",
    "        print(\"Generate 3-gram...\")\n",
    "        df[\"text_3gram\"] = list(df.apply(lambda x: self.Myngrams(self.TextPreProcessing(x[\"text\"]), 3), axis=1))\n",
    "        return df\n",
    "\n",
    "    def DumpTextCountFeatures(self, df):\n",
    "        ################################\n",
    "        ## text count and digit count ##\n",
    "        ################################\n",
    "        print(\"Generate basic text count features...\")\n",
    "        fnames = [\"text\"]\n",
    "        ngrams = [\"1gram\", \"2gram\", \"3gram\"]\n",
    "        CountDigit = lambda x: sum([1. for w in x if w.isdigit()])\n",
    "    \n",
    "        for fname in fnames:\n",
    "            for ngram in ngrams:\n",
    "                ## word count\n",
    "                df[\"Count_%s_%s\"%(fname, ngram)] = list(df.apply(lambda x: len(x[fname + \"_\" + ngram]), axis=1))\n",
    "                df[\"CountUnique_%s_%s\"%(fname, ngram)] = list(df.apply(lambda x: len(set(x[fname + \"_\" + ngram])), axis=1))\n",
    "                df[\"RatioUnique_%s_%s\"%(fname, ngram)] = list(df.apply(lambda x: self.TryDivide(x[\"CountUnique_%s_%s\"%(fname, ngram)], x[\"Count_%s_%s\"%(fname, ngram)]), axis=1))\n",
    "            ## digit count\n",
    "            df[\"CountDigit_%s\"%fname] = list(df.apply(lambda x: CountDigit(x[fname + \"_1gram\"]), axis=1))\n",
    "            df[\"RatioDigit_%s\"%fname] = list(df.apply(lambda x: self.TryDivide(x[\"CountDigit_%s\"%fname], x[\"Count_%s_1gram\"%(fname)]), axis=1))\n",
    "        '''\n",
    "        ## description missing indicator\n",
    "        #df[\"description_missing\"] = list(df.apply(lambda x: int(x[\"description_unigram\"] == \"\"), axis=1))\n",
    "        '''\n",
    "\n",
    "        df.drop(['text_1gram', 'text_2gram', 'text_3gram'], axis=1, inplace=True)\n",
    "    \n",
    "        return df    \n",
    "    \n",
    "    def GenTrainDataset(self):\n",
    "        #merge review and bussiness, based on review\n",
    "        sub_df_b = self.df_b[['state', 'stars', 'review_count']]\n",
    "        sub_df_b.rename(index=str, columns={\"stars\": \"bstar_ave\", \"review_count\": \"breview_count\"}, inplace=True)\n",
    "        df_train_r_comb_b = pd.merge(self.df_r, sub_df_b, how='outer', left_on='business_id' , right_index=True)\n",
    "    \n",
    "        #merge review and user, based on review\n",
    "        sub_df_u = self.df_u[['fans', 'review_count', 'friends']]\n",
    "        sub_df_u.rename(index=str, columns={\"review_count\": \"ureview_count\"}, inplace=True)\n",
    "        sub_df_u['nfriends'] = list(sub_df_u.apply(lambda x: len(x['friends']), axis=1))\n",
    "        sub_df_u.drop(['friends'], axis=1, inplace=True)\n",
    "        df_train_r_comb_b_u = pd.merge(df_train_r_comb_b, sub_df_u, how='outer', left_on='user_id', right_index=True)\n",
    "\n",
    "        #build nlp features...\n",
    "        df_train = self.DumpTextCountFeatures( self.DumpTextBasicNgram(df_train_r_comb_b_u) )\n",
    "        df_train.drop(['date', 'text', 'business_id', 'user_id', 'state'], axis=1, inplace=True)\n",
    "        return df_train\n",
    "\n",
    "sub_df_review = sklearn.utils.shuffle(df_review).iloc[:50000, :]\n",
    "preprocessing = PreProcessing(sub_df_review, df_business, df_user)\n",
    "df_train = preprocessing.GenTrainDataset()\n",
    "print (\"done with train set preprocessing!\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
